{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Affinity: [22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n"
     ]
    }
   ],
   "source": [
    "# Set the CPU affinity to specific cores: 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Get the current process\n",
    "current_process = psutil.Process(os.getpid())\n",
    "\n",
    "# Set CPU affinity to the specified cores\n",
    "current_process.cpu_affinity([22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32])\n",
    "\n",
    "# Verify if the CPU affinity is set correctly\n",
    "print(f\"CPU Affinity: {current_process.cpu_affinity()}\")\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Generate a timestamp for the log file\n",
    "time_stamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "log_file = f\"train-{time_stamp}.log\"\n",
    "\n",
    "# Set up the logger\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum import attr\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from WFlib import models\n",
    "from WFlib.tools import data_processor, analyzer\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvBlock2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2D convolutional block consisting of two convolutional layers followed by batch normalization\n",
    "    and ReLU activation, with a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(ConvBlock2d, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=\"same\"\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=\"same\"\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.downsample = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "        self.last_relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.last_relu(out + res)\n",
    "\n",
    "\n",
    "class Encoder2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2D convolutional encoder consisting of multiple ConvBlock2d layers followed by max pooling and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, conv_num_layers):\n",
    "        super(Encoder2d, self).__init__()\n",
    "        layers = []\n",
    "        cur_in_channels = in_channels\n",
    "        cur_out_channels = 16\n",
    "        for i in range(conv_num_layers):\n",
    "            layers.append(ConvBlock2d(cur_in_channels, cur_out_channels, (3, 7)))\n",
    "            if i < conv_num_layers - 1:\n",
    "                layers.append(nn.MaxPool2d((1, 2)))\n",
    "            else:\n",
    "                layers.append(nn.MaxPool2d((2, 2)))\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "            cur_in_channels = cur_out_channels\n",
    "            cur_out_channels = cur_out_channels * 2\n",
    "            if i == conv_num_layers - 2:\n",
    "                cur_out_channels = out_channels\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer encoder block consisting of multi-head attention, feed-forward layers, and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.2):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (seq_len, batch_size, embed_dim)\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.layernorm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention pooling module to aggregate sequence information into a single vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.attn = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, embed_dim)\n",
    "        weights = F.softmax(self.attn(x), dim=1)  # (batch_size, seq_len, 1)\n",
    "        return torch.sum(x * weights, dim=1)  # (batch_size, embed_dim)\n",
    "\n",
    "\n",
    "class CTFNet(nn.Module):\n",
    "    \"\"\"\n",
    "    An improved multi-scale time series network with convolutional encoders, Transformer encoders,\n",
    "    attention pooling, and classification/projection heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, num_scales=3, num_features=5, sub_dim=2, seq_len=1000, embed_dim=64, num_heads=4, ff_dim=128):\n",
    "        super(CTFNet, self).__init__()\n",
    "        \n",
    "        self.num_scales = num_scales\n",
    "        self.num_features = num_features\n",
    "        self.sub_dim = sub_dim\n",
    "        \n",
    "        # Learnable positional encodings\n",
    "        seq_len_after = 125\n",
    "        self.pos_encodings = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(1, seq_len_after, embed_dim))\n",
    "            for _ in range(num_scales)\n",
    "        ])\n",
    "        \n",
    "        # Initialize positional encodings\n",
    "        for pos_enc in self.pos_encodings:\n",
    "            nn.init.trunc_normal_(pos_enc, std=0.02)\n",
    "\n",
    "        # Use Encoder2d to process data for each scale\n",
    "        self.encoder2d_blocks = nn.ModuleList([\n",
    "            Encoder2d(in_channels=num_features, out_channels=embed_dim, conv_num_layers=3)\n",
    "            for _ in range(num_scales)\n",
    "        ])\n",
    "        \n",
    "        # Transformer encoders\n",
    "        self.transformer_encoders = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_scales)\n",
    "        ])\n",
    "        \n",
    "        # Multi-head self-attention fusion\n",
    "        self.fusion_attn = nn.MultiheadAttention(embed_dim * num_scales, num_heads, dropout=0.3)\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.attn_pool = AttentionPooling(embed_dim * num_scales)\n",
    "        \n",
    "        # Projection head for contrastive learning\n",
    "        projection_dim = 64\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim * num_scales, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, projection_dim)\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim * num_scales, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, num_scales, num_features, sub_dim, seq_len)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Multi-scale 2D convolution processing\n",
    "        conv_outputs = []\n",
    "        for i in range(self.num_scales):\n",
    "            # Get data for the current scale: (batch_size, num_features, sub_dim, seq_len)\n",
    "            scale_x = x[:, i]  \n",
    "            # Pass through Encoder2d: output will be (batch_size, embed_dim, 1, 125)\n",
    "            conv_out = self.encoder2d_blocks[i](scale_x)\n",
    "            \n",
    "            # Reshape to fit subsequent processing: (batch_size, embed_dim, 125)\n",
    "            conv_out = conv_out.squeeze(2)\n",
    "            \n",
    "            conv_outputs.append(conv_out)\n",
    "        \n",
    "        # Adjust dimensions for Transformer and add positional encodings\n",
    "        transformer_inputs = []\n",
    "        for i, conv_out in enumerate(conv_outputs):\n",
    "            # Reshape convolution output to (batch_size, seq_len, embed_dim)\n",
    "            t_input = conv_out.permute(0, 2, 1)\n",
    "            \n",
    "            # Add positional encodings\n",
    "            t_input = t_input + self.pos_encodings[i]\n",
    "            \n",
    "            # Convert to Transformer expected input shape: (seq_len, batch_size, embed_dim)\n",
    "            t_input = t_input.permute(1, 0, 2)\n",
    "            \n",
    "            transformer_inputs.append(t_input)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        transformer_outputs = []\n",
    "        for i in range(self.num_scales):\n",
    "            trans_out = self.transformer_encoders[i](transformer_inputs[i])  # (seq_len, batch_size, embed_dim)\n",
    "            transformer_outputs.append(trans_out)\n",
    "        \n",
    "        # Multi-scale fusion\n",
    "        fused = torch.cat(transformer_outputs, dim=2)  # (seq_len, batch_size, embed_dim * num_scales)\n",
    "        fused, _ = self.fusion_attn(fused, fused, fused)  # (seq_len, batch_size, embed_dim * num_scales)\n",
    "        \n",
    "        # Adjust dimensions for attention pooling\n",
    "        fused = fused.permute(1, 0, 2)  # (batch_size, seq_len, embed_dim * num_scales)\n",
    "        \n",
    "        # Attention pooling\n",
    "        pooled = self.attn_pool(fused)  # (batch_size, embed_dim * num_scales)\n",
    "\n",
    "        # Projection head\n",
    "        projection = self.projection_head(pooled)  # (batch_size, projection_dim)\n",
    "        projection = F.normalize(projection, p=2, dim=1)\n",
    "        \n",
    "        # Fully connected layer output\n",
    "        output = self.classifier(pooled)  # (batch_size, num_classes)\n",
    "        \n",
    "        return output, projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning.losses import SupConLoss\n",
    "import time\n",
    "\n",
    "def train_ctf_model(X_train, y_train, X_val, y_val, num_classes, epochs=50, \n",
    "                     batch_size=128, val_batch_size=128, save_dir='./checkpoints',\n",
    "                     resume_from=None):\n",
    "    \"\"\"\n",
    "    Train the ctf model, supporting batch evaluation, best model saving, and training resumption.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_val, y_val: Validation data and labels\n",
    "        num_classes: Number of classes\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Training batch size\n",
    "        val_batch_size: Validation batch size\n",
    "        save_dir: Directory to save the model\n",
    "        resume_from: Path to resume training from a checkpoint\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    \n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Data preparation\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    y_val = torch.LongTensor(y_val)\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Create validation data loader\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    #model = Groupctf(num_classes=num_classes, feature_dim=10, seq_len=1000)\n",
    "    #model = ctf(num_classes=num_classes, feature_dim=10, seq_len=1000)\n",
    "    model = CTFNet(num_classes=num_classes)\n",
    " \n",
    "    \n",
    "    # Set to cuda:1\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer and loss functions\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    ce_criterion = nn.CrossEntropyLoss()\n",
    "    sc_criterion = SupConLoss(temperature=0.07)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.6, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Initialize training state\n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Resume training (if checkpoint is provided)\n",
    "    if resume_from and os.path.isfile(resume_from):\n",
    "        logging.info(f\"Resuming from checkpoint: {resume_from}\")\n",
    "        checkpoint = torch.load(resume_from)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_acc = checkpoint['best_val_acc']\n",
    "        best_epoch = checkpoint['best_epoch']\n",
    "        logging.info(f\"Resuming from epoch {start_epoch}, best accuracy: {best_val_acc*100:.2f}% (epoch {best_epoch})\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, projection = model(inputs)\n",
    "            ce_loss = ce_criterion(outputs, targets)\n",
    "            sc_loss = sc_criterion(projection, targets)\n",
    "            loss = ce_loss + sc_loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation phase - batch evaluation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\"):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs, projection = model(inputs)\n",
    "\n",
    "                ce_loss = ce_criterion(outputs, targets)\n",
    "                #sc_loss = sc_criterion(projection, targets)\n",
    "                \n",
    "                loss = ce_loss #+ sc_loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), f\"{save_dir}/best_ctf_model.pt\")\n",
    "            logging.info(f\"Saved best model, validation accuracy: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        # Save checkpoint every 5 epochs or at the last epoch\n",
    "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'best_epoch': best_epoch\n",
    "            }\n",
    "            torch.save(checkpoint, f\"{save_dir}/checkpoint_epoch_{epoch+1}.pt\")\n",
    "            logging.info(f\"Saved checkpoint: epoch_{epoch+1}\")\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        scheduler.step(val_loss / len(val_loader))\n",
    "        \n",
    "        # Print statistics\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        logging.info(f\"Epoch {epoch+1}/{epochs} - Time: {epoch_time:.1f}s\")\n",
    "        logging.info(f\"Training Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "              f\"Training Accuracy: {train_acc*100:.2f}%\")\n",
    "        logging.info(f\"Validation Loss: {val_loss/len(val_loader):.4f} | \"\n",
    "              f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "        logging.info(f\"Best Validation Accuracy: {best_val_acc*100:.2f}% (Epoch {best_epoch+1})\")\n",
    "        logging.info(\"-\" * 60)\n",
    "    \n",
    "    # Load the best model after training\n",
    "    best_model_path = f\"{save_dir}/best_ctf_model.pt\"\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        logging.info(f\"Loaded best model (Epoch {best_epoch+1}, Accuracy: {best_val_acc*100:.2f}%)\")\n",
    "    \n",
    "    return model, best_val_acc, best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def fast_count_burst(arr):\n",
    "    diff = np.diff(arr)\n",
    "    change_indices = np.nonzero(diff)[0]\n",
    "    segment_starts = np.insert(change_indices + 1, 0, 0)\n",
    "    segment_ends = np.append(change_indices, len(arr) - 1)\n",
    "    segment_lengths = segment_ends - segment_starts + 1\n",
    "    segment_signs = np.sign(arr[segment_starts])\n",
    "    adjusted_lengths = segment_lengths * segment_signs\n",
    "\n",
    "    return adjusted_lengths\n",
    "\n",
    "def process_HMTF(index, sequence, intervals, max_lens):\n",
    "    packets = np.trim_zeros(sequence, \"fb\")\n",
    "    if len(packets) == 0:\n",
    "        return index, np.zeros((len(intervals), 10, max(max_lens)))\n",
    "        \n",
    "    abs_packets = np.abs(packets)\n",
    "    st_time = abs_packets[0]\n",
    "    \n",
    "    hmtf = np.zeros((len(intervals), 10, max(max_lens)))\n",
    "    \n",
    "    for scale_idx, (interval, max_len) in enumerate(zip(intervals, max_lens)):\n",
    "        st_pos = 0\n",
    "        \n",
    "        for interval_idx in range(max_len):\n",
    "            ed_time = (interval_idx + 1) * interval\n",
    "            if interval_idx == max_len - 1:\n",
    "                ed_pos = abs_packets.shape[0]\n",
    "            else:\n",
    "                ed_pos = np.searchsorted(abs_packets, st_time + ed_time)\n",
    "            \n",
    "            #print(st_pos, ed_pos)\n",
    "            if st_pos < ed_pos:\n",
    "                cur_packets = packets[st_pos:ed_pos]\n",
    "                \n",
    "                # 1-2: Directional statistics\n",
    "                out_packets = cur_packets[cur_packets > 0]\n",
    "                in_packets = cur_packets[cur_packets < 0]\n",
    "                hmtf[scale_idx, 0, interval_idx] = len(out_packets)  # Number of outgoing packets\n",
    "                hmtf[scale_idx, 1, interval_idx] = len(in_packets)   # Number of incoming packets\n",
    "                \n",
    "\n",
    "                \n",
    "                # 3-6: Burst analysis\n",
    "                if len(cur_packets) > 0:\n",
    "                    dirs = np.sign(cur_packets)\n",
    "                    bursts = fast_count_burst(dirs)\n",
    "                    out_bursts = bursts[bursts > 0]\n",
    "                    in_bursts = np.abs(bursts[bursts < 0])\n",
    "                    \n",
    "                    hmtf[scale_idx, 2, interval_idx] = len(out_bursts)  # Number of outgoing bursts\n",
    "                    hmtf[scale_idx, 3, interval_idx] = len(in_bursts)   # Number of incoming bursts\n",
    "                    hmtf[scale_idx, 4, interval_idx] = np.mean(out_bursts) if len(out_bursts) > 0 else 0  # Average outgoing burst length\n",
    "                    hmtf[scale_idx, 5, interval_idx] = np.mean(in_bursts) if len(in_bursts) > 0 else 0    # Average incoming burst length\n",
    "                \n",
    "                # 7-8: Time interval statistics\n",
    "                if len(out_packets) > 1:\n",
    "                    out_times = np.diff(abs_packets[st_pos:ed_pos][cur_packets > 0])\n",
    "                    hmtf[scale_idx, 6, interval_idx] = np.mean(out_times) if len(out_times) > 0 else 0  # Average time between outgoing packets\n",
    "                \n",
    "                if len(in_packets) > 1:\n",
    "                    in_times = np.diff(abs_packets[st_pos:ed_pos][cur_packets < 0])\n",
    "                    hmtf[scale_idx, 7, interval_idx] = np.mean(in_times) if len(in_times) > 0 else 0    # Average time between incoming packets\n",
    "                \n",
    "                # 9: Outgoing to incoming ratio\n",
    "                total_packets = len(out_packets) + len(in_packets)\n",
    "                hmtf[scale_idx, 8, interval_idx] = len(out_packets)/total_packets if total_packets > 0 else 0\n",
    "\n",
    "                # 10: Time density - packets per millisecond\n",
    "                interval_duration = abs_packets[ed_pos-1] - abs_packets[st_pos] if ed_pos > st_pos else interval\n",
    "                hmtf[scale_idx, 9, interval_idx] = len(cur_packets) / interval_duration if interval_duration > 0 else 0\n",
    "                \n",
    "            st_pos = ed_pos\n",
    "    \n",
    "    return index, hmtf\n",
    "\n",
    "def extract_HMTF(sequences, num_workers=30):\n",
    "    \"\"\"Extract hybrid multi-scale traffic features from sequences\"\"\"\n",
    "    # Define multiple time scales: first 50ms, first 80ms, first 120ms\n",
    "    intervals = [50, 100, 200]\n",
    "    max_lens = [1000, 800, 600]\n",
    "    \n",
    "    sequences *= 1000  # Convert to milliseconds\n",
    "    num_sequences = sequences.shape[0]\n",
    "    \n",
    "    # Final feature shape: [num_sequences, num_scales, num_features, max_length]\n",
    "    hmtf = np.zeros((num_sequences, len(intervals), 10, max(max_lens)))\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=min(num_workers, num_sequences)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_HMTF, index, sequences[index], intervals, max_lens\n",
    "            ) for index in range(num_sequences)\n",
    "        ]\n",
    "        with tqdm(total=num_sequences) as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                index, result = future.result()\n",
    "                hmtf[index] = result\n",
    "                pbar.update(1)\n",
    "\n",
    "    return hmtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_HMTF\n",
    "# Load the dataset\n",
    "# train_data = np.load(\"datasets/OW/train.npz\")\n",
    "# valid_data = np.load(\"datasets/OW/valid.npz\")\n",
    "# test_data = np.load(\"datasets/OW/test.npz\")\n",
    "\n",
    "# X_train, y_train = train_data[\"X\"], train_data[\"y\"]\n",
    "# X_val, y_val = valid_data[\"X\"], valid_data[\"y\"]\n",
    "# X_test, y_test = test_data[\"X\"], test_data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train =  data_processor.length_align(X_train, 5000)\n",
    "# X_val =  data_processor.length_align(X_val, 5000)\n",
    "# X_test =  data_processor.length_align(X_test, 5000)\n",
    "# X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_HMTF = extract_HMTF(X_train)\n",
    "# X_val_HMTF = extract_HMTF(X_val)\n",
    "# X_test_HMTF = extract_HMTF(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to npz\n",
    "# np.savez(\"datasets/OW/train_HMTF.npz\", X=X_train_HMTF, y=y_train)\n",
    "# np.savez(\"datasets/OW/valid_HMTF.npz\", X=X_val_HMTF, y=y_val)\n",
    "# np.savez(\"datasets/OW/test_HMTF.npz\", X=X_test_HMTF, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = np.load(\"datasets/CW/train_HMTF.npz\")\n",
    "valid_data = np.load(\"datasets/CW/valid_HMTF.npz\")\n",
    "test_data = np.load(\"datasets/CW/test_HMTF.npz\")\n",
    "X_train_HMTF, y_train = train_data[\"X\"], train_data[\"y\"]\n",
    "X_val_HMTF, y_val = valid_data[\"X\"], valid_data[\"y\"]\n",
    "X_test_HMTF, y_test = test_data[\"X\"], test_data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85641, 3, 10, 1000) (85641,) (9516, 3, 10, 1000) (9516,) (10573, 3, 10, 1000) (10573,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_HMTF.shape, y_train.shape, X_val_HMTF.shape, y_val.shape, X_test_HMTF.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85641, 3, 5, 2, 1000) (85641,) (9516, 3, 5, 2, 1000) (9516,)\n"
     ]
    }
   ],
   "source": [
    "# reshape (85641, 3, 10, 1000) to (85641, 3, 5, 2,1000)\n",
    "X_train_HMTF_reshaped = X_train_HMTF.reshape(85641, 3, 5, 2, 1000)\n",
    "X_val_HMTF_reshaped = X_val_HMTF.reshape(9516, 3, 5, 2, 1000)\n",
    "\n",
    "print(X_train_HMTF_reshaped.shape, y_train.shape, X_val_HMTF_reshaped.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "model, best_acc, best_epoch = train_ctf_model(\n",
    "    X_train_HMTF_reshaped, y_train, X_val_HMTF_reshaped, y_val, \n",
    "    num_classes=num_classes, \n",
    "    epochs=60,\n",
    "    save_dir='./ctf_train',\n",
    "    batch_size=128,\n",
    "    #resume_from='./ModelV1.7-2m-OW/checkpoint_epoch_40.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CTFNet(num_classes=len(np.unique(y_test)))\n",
    "#checkpoint = torch.load(\"./ModelV1.7/checkpoint_epoch_60.pt\")\n",
    "model.load_state_dict(torch.load(\"./ctf_train/best_ctf_model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10573, 3, 10, 1000) (10573,)\n",
      "(10573, 3, 5, 2, 1000) (10573,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_HMTF.shape, y_test.shape)\n",
    "X_test_HMTF_reshaped = X_test_HMTF.reshape(10573, 3, 5, 2, 1000)\n",
    "print(X_test_HMTF_reshaped.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def evaluate_ctf_model(model, X_test, y_test, batch_size=64, device=None):\n",
    "    \"\"\"\n",
    "    Batch evaluation of the ctf model to avoid memory overflow.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained ctf model\n",
    "    X_test: Test data [num_samples, 3, 10, 1000]\n",
    "    y_test: Test labels\n",
    "    batch_size: Batch size\n",
    "    device: Computation device\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing various evaluation metrics\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Convert to Tensor\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.LongTensor(y_test)\n",
    "    \n",
    "    # Create data loader\n",
    "    test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Store predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    #all_attention_weights = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Model prediction\n",
    "            outputs, attention_weights = model(inputs)\n",
    "            _, predictions = outputs.max(1)\n",
    "            \n",
    "            # Collect results\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "            #all_attention_weights.append(attention_weights.cpu().numpy())\n",
    "    \n",
    "    # Convert to NumPy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    #all_attention_weights = np.concatenate(all_attention_weights, axis=0)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    \n",
    "    # Compute average attention weights for each scale\n",
    "    #avg_scale_attention = all_attention_weights.mean(axis=0)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Create evaluation results dictionary\n",
    "    results = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm,\n",
    "        #\"avg_scale_attention\": avg_scale_attention\n",
    "    }\n",
    "    \n",
    "    # Print main evaluation results\n",
    "    print(f\"Test accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Macro average precision: {precision:.4f}\")\n",
    "    print(f\"Macro average recall: {recall:.4f}\")\n",
    "    print(f\"Macro average F1 score: {f1:.4f}\")\n",
    "    #print(f\"Scale attention weights: {avg_scale_attention}\")\n",
    "\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "# model = train_ctf_model(X_train, y_train, X_val, y_val, num_classes, epochs=50)\n",
    "# eval_results = evaluate_ctf_model(model, X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_ctf_model(model, X_test_HMTF_reshaped, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xjx_WF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
